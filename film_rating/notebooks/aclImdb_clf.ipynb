{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c8f8b1a-0ce9-45a8-bf4f-bf64dbad42d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost.text_processing import Tokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import Pool, CatBoostClassifier, CatBoostRegressor\n",
    "from catboost.utils import eval_metric\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "DATA_DIR = \"../datasets\"\n",
    "MODEL_DIR = \"../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef3afefe-3ca9-4af0-8839-7b4a865b3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = os.path.join(DATA_DIR, \"aclImdb_v1.tar.gz\")\n",
    "\n",
    "if not os.path.exists(source):\n",
    "    !wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P $DATA_DIR\n",
    "\n",
    "if not os.path.exists(\"/tmp/aclImdb\"):\n",
    "    !tar -xf $source -C /tmp\n",
    "\n",
    "train_dir = \"/tmp/aclImdb/train\"\n",
    "test_dir = \"/tmp/aclImdb/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4eb83b8-454e-473d-9f8e-0fdefb113ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path):\n",
    "    review = []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        files = os.listdir(os.path.join(path, label))\n",
    "        for file in files:\n",
    "            with open(os.path.join(path, label, file)) as f:\n",
    "                text = f.readline()\n",
    "                rating = file.split(\".\")[0].split(\"_\")[1]\n",
    "                review.append([text, label, int(rating)])\n",
    "    df = pd.DataFrame(review, columns=[\"text\", \"label\", \"rating\"])\n",
    "    return df\n",
    "\n",
    "train_df = extract_data(train_dir)\n",
    "test_df = extract_data(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00764b6-3929-4c34-943c-56cb9b40aca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 3), (5000, 3), (25000, 3))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ImdbDataset():\n",
    "    def __init__(self, vocab=None):\n",
    "        super().__init__()\n",
    "        self.tokenizer = Tokenizer(lowercasing=True, separator_type='BySense', token_types=['Word', 'Number'])\n",
    "        nltk.download([\"stopwords\", \"wordnet\"], quiet=True)\n",
    "        self.stop_words = stopwords.words(\"english\")\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def _stop_words_filter(self, tokens):\n",
    "        return list(filter(lambda x: x not in self.stop_words, tokens))\n",
    "\n",
    "    def _lemmatize(self, tokens):\n",
    "        return list(map(lambda t: self.lemmatizer.lemmatize(t), tokens))\n",
    "\n",
    "    def preprocess_text(self, texts):\n",
    "        tokenized_text = [self.tokenizer.tokenize(text) for text in texts]\n",
    "        tokenized_no_stop = [self._stop_words_filter(tokens) for tokens in tokenized_text]\n",
    "        lemmatized_text = [\" \".join(self._lemmatize(tokens)) for tokens in tokenized_no_stop]\n",
    "        return lemmatized_text\n",
    "\n",
    "    def from_df(self, data):\n",
    "        return pd.DataFrame({\n",
    "            \"text\": self.preprocess_text(data.text),\n",
    "            \"label\": data.label,\n",
    "            \"rating\": data.rating,\n",
    "        })\n",
    "\n",
    "dataset = ImdbDataset()\n",
    "\n",
    "train_df = dataset.from_df(train_df)\n",
    "test_df = dataset.from_df(test_df)\n",
    "\n",
    "train_df, \\\n",
    "val_df = train_test_split(train_df, stratify=train_df.label, test_size=0.2, random_state=SEED)\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac647cd5-e8ae-4bdb-8c0b-729ac4127c05",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0ab5adf-79a6-451a-9bc4-cdaf984c85bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.0552\n",
      "0:\tlearn: 0.8524500\ttest: 0.8596000\tbest: 0.8596000 (0)\ttotal: 143ms\tremaining: 2m 22s\n",
      "100:\tlearn: 0.8738000\ttest: 0.8760000\tbest: 0.8760000 (98)\ttotal: 3.55s\tremaining: 31.6s\n",
      "200:\tlearn: 0.8847000\ttest: 0.8826000\tbest: 0.8826000 (200)\ttotal: 7.19s\tremaining: 28.6s\n",
      "300:\tlearn: 0.8935500\ttest: 0.8878000\tbest: 0.8878000 (300)\ttotal: 10.4s\tremaining: 24.1s\n",
      "400:\tlearn: 0.9005000\ttest: 0.8920000\tbest: 0.8922000 (391)\ttotal: 13.5s\tremaining: 20.2s\n",
      "500:\tlearn: 0.9065500\ttest: 0.8942000\tbest: 0.8944000 (483)\ttotal: 16.5s\tremaining: 16.5s\n",
      "600:\tlearn: 0.9119000\ttest: 0.8978000\tbest: 0.8978000 (599)\ttotal: 19.5s\tremaining: 13s\n",
      "700:\tlearn: 0.9153500\ttest: 0.8988000\tbest: 0.8988000 (658)\ttotal: 22.5s\tremaining: 9.61s\n",
      "800:\tlearn: 0.9189500\ttest: 0.9000000\tbest: 0.9000000 (797)\ttotal: 25.5s\tremaining: 6.34s\n",
      "900:\tlearn: 0.9231500\ttest: 0.8994000\tbest: 0.9004000 (860)\ttotal: 28.6s\tremaining: 3.14s\n",
      "999:\tlearn: 0.9265500\ttest: 0.8994000\tbest: 0.9004000 (860)\ttotal: 31.5s\tremaining: 0us\n",
      "bestTest = 0.9004\n",
      "bestIteration = 860\n",
      "Shrink model to first 861 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87556"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pool = Pool(\n",
    "    train_df[[\"text\"]], \n",
    "    train_df.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "val_pool = Pool(\n",
    "    val_df[[\"text\"]],\n",
    "    val_df.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "test_pool = Pool(\n",
    "    test_df[[\"text\"]],\n",
    "    test_df.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "clf = CatBoostClassifier(\n",
    "    eval_metric=\"Accuracy\", \n",
    "    task_type=\"GPU\",\n",
    "    random_seed=SEED, \n",
    ")\n",
    "clf.fit(train_pool, eval_set=val_pool, verbose=100)    \n",
    "clf.score(test_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b32b24-3636-4caa-b743-d59b400cfca0",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e459efdc-f725-4019-86eb-1b371cfcf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72695073-ceee-4af9-b52f-25237274495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"lvwerra/distilbert-imdb\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, clean_up_tokenization_spaces=True)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    label_mapping = {\"neg\": 0, \"pos\": 1}\n",
    "    inputs = tokenizer(examples[\"text\"], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    label = [label_mapping[label] for label in examples[\"label\"]]\n",
    "    return Dataset.from_dict({\n",
    "        \"text\": examples[\"text\"],\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"label\": label,\n",
    "        \"rating\": examples[\"rating\"],\n",
    "    })\n",
    "train_ds = preprocess_function(Dataset.from_dict(train_df))\n",
    "val_ds = preprocess_function(Dataset.from_dict(val_df))\n",
    "test_ds = preprocess_function(Dataset.from_dict(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffab8fe6-0873-428b-91bf-43a759957b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, bs=4, device=DEVICE):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor(data[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(data[\"attention_mask\"]).to(device)\n",
    "    logits = []\n",
    "    for i in tqdm(range(0, len(data), bs)):\n",
    "        batch = range(i, i+bs)\n",
    "        out = model(input_ids[batch], attention_mask=attention_mask[batch])\n",
    "        logits.append(out.logits.data.cpu().detach().numpy())\n",
    "    return np.vstack(logits), data[\"label\"]\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb81c64-ee0d-4d4d-a595-2f7ba9f382a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c653a54b7b84cc6832ac9f493e3feaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1233f63264047a9b19c215dc2162846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f10560dad54e2a87851ce63b0ba204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02190f8b64e24e2487fd1b0af9e4e046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.84876}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "compute_metrics(predict(model, test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bbcd787-94cb-42aa-a6a0-a7866f1d7818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 24:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.313200</td>\n",
       "      <td>0.301227</td>\n",
       "      <td>0.895400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.299281</td>\n",
       "      <td>0.908400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.34711530804634094,\n",
       " 'eval_accuracy': 0.89792,\n",
       " 'eval_runtime': 571.2856,\n",
       " 'eval_samples_per_second': 43.761,\n",
       " 'eval_steps_per_second': 10.94,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "modules = [\n",
    "    model.classifier,\n",
    "    model.pre_classifier,\n",
    "    model.distilbert.transformer.layer[-1:],\n",
    "]\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for module in modules:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"bert_results\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08f9688a-a491-4101-b699-9a5230caafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(os.path.join(MODEL_DIR, \"hf_clf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b164b-edf4-4ebc-afe0-5fb01740bbaf",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23db74ed-f557-42d8-af33-9103cbdceb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.0552\n",
      "0:\tlearn: 0.9197000\ttest: 0.9072000\tbest: 0.9072000 (0)\ttotal: 35.6ms\tremaining: 35.5s\n",
      "100:\tlearn: 0.9280500\ttest: 0.9154000\tbest: 0.9158000 (78)\ttotal: 3.08s\tremaining: 27.5s\n",
      "200:\tlearn: 0.9317000\ttest: 0.9150000\tbest: 0.9162000 (106)\ttotal: 6.23s\tremaining: 24.8s\n",
      "300:\tlearn: 0.9351000\ttest: 0.9158000\tbest: 0.9162000 (106)\ttotal: 9.24s\tremaining: 21.5s\n",
      "400:\tlearn: 0.9374000\ttest: 0.9160000\tbest: 0.9166000 (380)\ttotal: 12.3s\tremaining: 18.3s\n",
      "500:\tlearn: 0.9406000\ttest: 0.9168000\tbest: 0.9170000 (490)\ttotal: 15.2s\tremaining: 15.2s\n",
      "600:\tlearn: 0.9431500\ttest: 0.9172000\tbest: 0.9174000 (589)\ttotal: 18.2s\tremaining: 12.1s\n",
      "700:\tlearn: 0.9454000\ttest: 0.9170000\tbest: 0.9178000 (632)\ttotal: 21.2s\tremaining: 9.05s\n",
      "800:\tlearn: 0.9470500\ttest: 0.9170000\tbest: 0.9178000 (632)\ttotal: 24.2s\tremaining: 6.02s\n",
      "900:\tlearn: 0.9492500\ttest: 0.9176000\tbest: 0.9178000 (632)\ttotal: 27.2s\tremaining: 2.99s\n",
      "999:\tlearn: 0.9509000\ttest: 0.9172000\tbest: 0.9178000 (632)\ttotal: 30.2s\tremaining: 0us\n",
      "bestTest = 0.9178\n",
      "bestIteration = 632\n",
      "Shrink model to first 633 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.90568"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_df.assign(logits=trainer.predict(train_ds).predictions[:,0])\n",
    "val = val_df.assign(logits=trainer.predict(val_ds).predictions[:,0])\n",
    "test = test_df.assign(logits=trainer.predict(test_ds).predictions[:,0])\n",
    "\n",
    "train_pool = Pool(\n",
    "    train[[\"text\", \"logits\"]], \n",
    "    train.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "val_pool = Pool(\n",
    "    val[[\"text\", \"logits\"]],\n",
    "    val.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "test_pool = Pool(\n",
    "    test[[\"text\", \"logits\"]],\n",
    "    test.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "clf = CatBoostClassifier(\n",
    "    eval_metric=\"Accuracy\", \n",
    "    task_type=\"GPU\",\n",
    "    random_seed=SEED, \n",
    ")\n",
    "clf.fit(train_pool, eval_set=val_pool, verbose=100)    \n",
    "clf.score(test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b633f6-4fbc-4972-9d65-e4c2101151ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_DIR, \"cb_clf.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f34cb-0050-4d2b-a92b-004fd30c95b4",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e8bf0ea-f09c-49f7-b4c0-fe46011bf628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d4c4d367bf47d0a249c027b6819c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755df154d0bb4e338420bcd2b1e1e990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8ee09f378f4e798871ff3b08eda912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% GPU memory available for training. Free: 2671.4375 Total: 5933.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.085827\n",
      "0:\tlearn: 3.2400829\ttest: 3.2300987\tbest: 3.2300987 (0)\ttotal: 20.6ms\tremaining: 20.6s\n",
      "100:\tlearn: 1.3818951\ttest: 1.6833442\tbest: 1.6833442 (100)\ttotal: 1.69s\tremaining: 15s\n",
      "200:\tlearn: 1.3375295\ttest: 1.6800193\tbest: 1.6798837 (197)\ttotal: 3.3s\tremaining: 13.1s\n",
      "300:\tlearn: 1.3110882\ttest: 1.6783720\tbest: 1.6780603 (275)\ttotal: 4.85s\tremaining: 11.3s\n",
      "400:\tlearn: 1.2901631\ttest: 1.6789310\tbest: 1.6779108 (313)\ttotal: 6.4s\tremaining: 9.56s\n",
      "500:\tlearn: 1.2687693\ttest: 1.6798937\tbest: 1.6779108 (313)\ttotal: 7.98s\tremaining: 7.94s\n",
      "600:\tlearn: 1.2492149\ttest: 1.6804701\tbest: 1.6779108 (313)\ttotal: 9.52s\tremaining: 6.32s\n",
      "700:\tlearn: 1.2317313\ttest: 1.6816909\tbest: 1.6779108 (313)\ttotal: 11.1s\tremaining: 4.74s\n",
      "800:\tlearn: 1.2173295\ttest: 1.6808316\tbest: 1.6779108 (313)\ttotal: 12.7s\tremaining: 3.14s\n",
      "900:\tlearn: 1.2034216\ttest: 1.6829679\tbest: 1.6779108 (313)\ttotal: 14.2s\tremaining: 1.56s\n",
      "999:\tlearn: 1.1914413\ttest: 1.6824874\tbest: 1.6779108 (313)\ttotal: 15.7s\tremaining: 0us\n",
      "bestTest = 1.677910834\n",
      "bestIteration = 313\n",
      "Shrink model to first 314 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7144155279856985"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = os.path.join(MODEL_DIR, \"hf_clf\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, \"cb_clf.pkl\"), \"rb\") as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "def predict_proba(ds):\n",
    "    return ds.to_pandas().drop([\"input_ids\", \"attention_mask\"], axis=1) \\\n",
    "    .assign(logits=predict(model, ds)[0][:,0]) \\\n",
    "    .assign(proba=lambda x: clf.predict_proba(x)[:,0])\n",
    "\n",
    "train = predict_proba(train_ds)\n",
    "val = predict_proba(val_ds)\n",
    "test = predict_proba(test_ds)\n",
    "\n",
    "train_pool = Pool(\n",
    "    train.drop([\"label\", \"rating\"], axis=1), \n",
    "    train.rating, \n",
    "    text_features=[\"text\"]\n",
    ")\n",
    "val_pool = Pool(\n",
    "    val.drop([\"label\", \"rating\"], axis=1), \n",
    "    val.rating, \n",
    "    text_features=[\"text\"]\n",
    ")\n",
    "test_pool = Pool(\n",
    "    test.drop([\"label\", \"rating\"], axis=1), \n",
    "    test.rating, \n",
    "    text_features=[\"text\"]\n",
    ")\n",
    "reg = CatBoostRegressor(\n",
    "    objective='RMSE', \n",
    "    task_type=\"GPU\",\n",
    "    random_seed=SEED, \n",
    ")\n",
    "reg.fit(train_pool, eval_set=val_pool, verbose=100)    \n",
    "reg.score(test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eae5bf35-1462-4745-894e-9d4cd2e48c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_DIR, \"cb_reg.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(reg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b64bd5-6bd5-4e9f-8e52-b8e6327b1bf6",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8385a950-3894-4a99-825e-76549716fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from catboost.text_processing import Tokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "MODEL_DIR = MODEL_DIR = \"../models\"\n",
    "\n",
    "cb_tokenizer = Tokenizer(lowercasing=True, separator_type=\"BySense\", token_types=[\"Word\", \"Number\"])\n",
    "stop_words = stopwords.words(\"english\")\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\", clean_up_tokenization_spaces=True)\n",
    "\n",
    "checkpoint = os.path.join(MODEL_DIR, \"hf_clf\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "model.eval()\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, \"cb_clf.pkl\"), \"rb\") as f:\n",
    "\tclf = pickle.load(f)\n",
    "with open(os.path.join(MODEL_DIR, \"cb_reg.pkl\"), \"rb\") as f:\n",
    "\treg = pickle.load(f)\n",
    "\n",
    "def predict(text):\n",
    "    text = \" \".join([lemmatizer.lemmatize(token) for token in cb_tokenizer.tokenize(text) if token not in stop_words])\n",
    "    inputs = hf_tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"]).logits[:,0].detach().numpy()\n",
    "    df = pd.DataFrame({\"text\": text, \"logits\": logits}).assign(proba=lambda x: clf.predict_proba(x)[:,0])\n",
    "    return clf.predict(df)[0], round(reg.predict(df)[0], 0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e0d75b-5b04-47a6-b909-ab8c58af9da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"\"\"\n",
    "I went and saw this movie last night after being coaxed to by a few friends of mine. \n",
    "I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. \n",
    "Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. \n",
    "The sign of a good movie is that it can toy with our emotions. This one did exactly that. \n",
    "The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. \n",
    "While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. \n",
    "This movie was great, and I suggest that you go see it before you judge.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83fee06-6910-40c4-93eb-e5ce79791c93",
   "metadata": {},
   "source": [
    "## Bert Classifier + Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c886ea7c-24bc-413a-9d6f-8d482dc7817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import evaluate\n",
    "from torcheval.metrics.functional import r2_score\n",
    "import torch\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ad24e75-0740-4b2c-82d1-a2cb794d7b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320972b961a44defa44e24efb27b6b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c125573a9f453fa6ee1497778ac3ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2657419f75eb43239c5d3be849e4940d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eed1ed9900034e499a174f103d85dfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"lvwerra/distilbert-imdb\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, clean_up_tokenization_spaces=True)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    label_mapping = {\"neg\": 0, \"pos\": 1}\n",
    "    inputs = tokenizer(examples[\"text\"], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    label = [label_mapping[label] for label in examples[\"label\"]]\n",
    "    rating = examples[\"rating\"]\n",
    "    return Dataset.from_dict({\n",
    "        \"text\": examples[\"text\"],\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"label\": list(zip(label, rating)),\n",
    "    })\n",
    "train_ds = preprocess_function(Dataset.from_dict(train_df))\n",
    "val_ds = preprocess_function(Dataset.from_dict(val_df))\n",
    "test_ds = preprocess_function(Dataset.from_dict(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea7cbfc5-6c60-4790-b817-266339203de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBert(torch.nn.Module):\n",
    "    def __init__(self, alpha=0.1):\n",
    "        super(CustomBert, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        checkpoint = \"lvwerra/distilbert-imdb\"\n",
    "        self.aclimdb = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=3, ignore_mismatched_sizes=True)\n",
    "\n",
    "    def custom_loss(self, logits, labels, alpha):\n",
    "        loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        crossentropy = loss_fct(logits[:,:2], labels[:,0].long())\n",
    "        loss_fct = torch.nn.MSELoss()\n",
    "        mse = loss_fct(logits[:,2], labels[:,1].float())\n",
    "        return crossentropy + alpha * mse\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        out = self.aclimdb(input_ids, attention_mask)\n",
    "        logits = out.get(\"logits\")\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.custom_loss(logits, labels, self.alpha)\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss, \n",
    "            logits=logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6d845a67-c3a5-4363-89b6-7fef6af5437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    metrics = {} # evaluate.list_evaluation_modules()\n",
    "    \n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits[:,:2], axis=1)\n",
    "    metrics[\"Accuracy\"] = accuracy.compute(predictions=predictions, references=labels[:,0])[\"accuracy\"]\n",
    "\n",
    "    metrics[\"R2\"] = r2_score(torch.tensor(logits[:,2]), torch.tensor(labels[:,1]))\n",
    "    \n",
    "    reg_metrics = [\"mae\", \"mape\"]\n",
    "    for metric in reg_metrics:\n",
    "        m = evaluate.load(metric)\n",
    "        metrics[metric] = m.compute(predictions=logits[:,2], references=labels[:,1])[metric]\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "527c32ca-a2ea-48ca-b5a3-94269f8614f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7dfa20cc9a644d99681b5272254b2ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0d1418f338c4110948c385ab15ac7ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at lvwerra/distilbert-imdb and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([2, 768]) in the checkpoint and torch.Size([3, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([2]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25000' max='25000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25000/25000 57:50, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>R2</th>\n",
       "      <th>Mae</th>\n",
       "      <th>Mape</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.721500</td>\n",
       "      <td>0.697520</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.683877</td>\n",
       "      <td>1.309519</td>\n",
       "      <td>0.469289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.615000</td>\n",
       "      <td>0.664986</td>\n",
       "      <td>0.909400</td>\n",
       "      <td>0.685919</td>\n",
       "      <td>1.297682</td>\n",
       "      <td>0.462998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.610500</td>\n",
       "      <td>0.647062</td>\n",
       "      <td>0.906400</td>\n",
       "      <td>0.724908</td>\n",
       "      <td>1.233699</td>\n",
       "      <td>0.457518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.538500</td>\n",
       "      <td>0.599571</td>\n",
       "      <td>0.912200</td>\n",
       "      <td>0.726675</td>\n",
       "      <td>1.173209</td>\n",
       "      <td>0.382932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>0.618814</td>\n",
       "      <td>0.913800</td>\n",
       "      <td>0.721487</td>\n",
       "      <td>1.176024</td>\n",
       "      <td>0.409328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f746337fac154078a3f9f6be7f03615b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "338f6eac82d044829fe9ec32ced98189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72c255d196e4e149d838cab9d167d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.68k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7032240629196167,\n",
       " 'eval_Accuracy': 0.90104,\n",
       " 'eval_R2': 0.6850587129592896,\n",
       " 'eval_mae': 1.238709961400032,\n",
       " 'eval_mape': 0.4228595787776462,\n",
       " 'eval_runtime': 565.0604,\n",
       " 'eval_samples_per_second': 44.243,\n",
       " 'eval_steps_per_second': 11.061,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomBert()\n",
    "modules = [\n",
    "    model.aclimdb.classifier,\n",
    "    model.aclimdb.pre_classifier,\n",
    "    model.aclimdb.distilbert.transformer.layer[-1:],\n",
    "]\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for module in modules:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"bert_results\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7633a1f8-b37b-4a79-adbb-e3c177442aec",
   "metadata": {},
   "source": [
    "trainer.save_model(os.path.join(MODEL_DIR, \"hf_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df29de9-e484-4ec1-84e1-2b4b9bd65fc1",
   "metadata": {},
   "source": [
    "### Ensamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "616ad38b-6d28-43ce-aa4e-76bf1e70f344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train = pd.concat([\n",
    "    train_df.reset_index(drop=True), \n",
    "    pd.DataFrame(trainer.predict(train_ds).predictions[:,1:])], \n",
    "axis=1)\n",
    "\n",
    "val = pd.concat([\n",
    "    val_df.reset_index(drop=True), \n",
    "    pd.DataFrame(trainer.predict(val_ds).predictions[:,1:])], \n",
    "axis=1)\n",
    "\n",
    "test = pd.concat([\n",
    "    test_df.reset_index(drop=True), \n",
    "    pd.DataFrame(trainer.predict(test_ds).predictions[:,1:])], \n",
    "axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7cc7a5a-9662-4e49-8789-8a2ef1435654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.0552\n",
      "0:\tlearn: 0.9356000\ttest: 0.9106000\tbest: 0.9106000 (0)\ttotal: 79.4ms\tremaining: 1m 19s\n",
      "100:\tlearn: 0.9381000\ttest: 0.9162000\tbest: 0.9164000 (94)\ttotal: 3.13s\tremaining: 27.9s\n",
      "200:\tlearn: 0.9406000\ttest: 0.9158000\tbest: 0.9166000 (113)\ttotal: 6.16s\tremaining: 24.5s\n",
      "300:\tlearn: 0.9425000\ttest: 0.9168000\tbest: 0.9168000 (293)\ttotal: 9.12s\tremaining: 21.2s\n",
      "400:\tlearn: 0.9446500\ttest: 0.9172000\tbest: 0.9176000 (352)\ttotal: 12.1s\tremaining: 18s\n",
      "500:\tlearn: 0.9473500\ttest: 0.9168000\tbest: 0.9176000 (352)\ttotal: 15s\tremaining: 15s\n",
      "600:\tlearn: 0.9490500\ttest: 0.9176000\tbest: 0.9178000 (590)\ttotal: 18s\tremaining: 12s\n",
      "700:\tlearn: 0.9509000\ttest: 0.9178000\tbest: 0.9182000 (675)\ttotal: 21.1s\tremaining: 8.98s\n",
      "800:\tlearn: 0.9529500\ttest: 0.9172000\tbest: 0.9182000 (675)\ttotal: 24.1s\tremaining: 5.99s\n",
      "900:\tlearn: 0.9544500\ttest: 0.9176000\tbest: 0.9182000 (675)\ttotal: 27s\tremaining: 2.96s\n",
      "999:\tlearn: 0.9563000\ttest: 0.9186000\tbest: 0.9190000 (971)\ttotal: 30s\tremaining: 0us\n",
      "bestTest = 0.919\n",
      "bestIteration = 971\n",
      "Shrink model to first 972 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.90508"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pool = Pool(\n",
    "    train.drop([\"label\", \"rating\"], axis=1), \n",
    "    train.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "val_pool = Pool(\n",
    "    val.drop([\"label\", \"rating\"], axis=1), \n",
    "    val.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "test_pool = Pool(\n",
    "    test.drop([\"label\", \"rating\"], axis=1), \n",
    "    test.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "clf = CatBoostClassifier(\n",
    "    eval_metric=\"Accuracy\", \n",
    "    task_type=\"GPU\",\n",
    "    random_seed=SEED, \n",
    ")\n",
    "clf.fit(train_pool, eval_set=val_pool, verbose=100)    \n",
    "clf.score(test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e173d8e7-5775-4cca-ae37-f04671350fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.085827\n",
      "0:\tlearn: 3.2436841\ttest: 3.2306124\tbest: 3.2306124 (0)\ttotal: 14.1ms\tremaining: 14.1s\n",
      "100:\tlearn: 1.5499776\ttest: 1.7196114\tbest: 1.7196114 (100)\ttotal: 1.37s\tremaining: 12.2s\n",
      "200:\tlearn: 1.5210331\ttest: 1.7170233\tbest: 1.7170233 (200)\ttotal: 2.7s\tremaining: 10.7s\n",
      "300:\tlearn: 1.4985524\ttest: 1.7147792\tbest: 1.7147613 (299)\ttotal: 4.01s\tremaining: 9.32s\n",
      "400:\tlearn: 1.4775583\ttest: 1.7153798\tbest: 1.7146457 (302)\ttotal: 5.24s\tremaining: 7.82s\n",
      "500:\tlearn: 1.4571793\ttest: 1.7146877\tbest: 1.7144264 (492)\ttotal: 6.47s\tremaining: 6.44s\n",
      "600:\tlearn: 1.4393728\ttest: 1.7145204\tbest: 1.7137396 (547)\ttotal: 7.79s\tremaining: 5.17s\n",
      "700:\tlearn: 1.4213290\ttest: 1.7162146\tbest: 1.7137396 (547)\ttotal: 9.06s\tremaining: 3.87s\n",
      "800:\tlearn: 1.4058374\ttest: 1.7163201\tbest: 1.7137396 (547)\ttotal: 10.3s\tremaining: 2.56s\n",
      "900:\tlearn: 1.3911314\ttest: 1.7163002\tbest: 1.7137396 (547)\ttotal: 11.6s\tremaining: 1.27s\n",
      "999:\tlearn: 1.3753141\ttest: 1.7169199\tbest: 1.7137396 (547)\ttotal: 12.8s\tremaining: 0us\n",
      "bestTest = 1.713739629\n",
      "bestIteration = 547\n",
      "Shrink model to first 548 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7151975382445561"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pool = Pool(\n",
    "    train.drop([\"label\", \"rating\"], axis=1), \n",
    "    train.rating, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "val_pool = Pool(\n",
    "    val.drop([\"label\", \"rating\"], axis=1), \n",
    "    val.rating, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "test_pool = Pool(\n",
    "    test.drop([\"label\", \"rating\"], axis=1), \n",
    "    test.rating, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "reg = CatBoostRegressor(\n",
    "    objective='RMSE', \n",
    "    task_type=\"GPU\",\n",
    "    random_seed=SEED, \n",
    ")\n",
    "reg.fit(train_pool, eval_set=val_pool, verbose=100)    \n",
    "reg.score(test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0bb39b6-cb5f-4748-91cf-55b1a3aa75e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_DIR, \"cb_clf-1.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(clf, f)\n",
    "with open(os.path.join(MODEL_DIR, \"cb_reg-1.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(reg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7682c3d8-b556-419c-9ff6-281ea1e90b53",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be2a0b-7831-4595-b56a-ab3dfa8fc5d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

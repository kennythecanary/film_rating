{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c8f8b1a-0ce9-45a8-bf4f-bf64dbad42d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost.text_processing import Tokenizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import Pool, CatBoostClassifier, CatBoostRegressor\n",
    "from catboost.utils import eval_metric\n",
    "from tqdm.auto import tqdm\n",
    "import pickle\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "DATA_DIR = \"../datasets\"\n",
    "MODEL_DIR = \"../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef3afefe-3ca9-4af0-8839-7b4a865b3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = os.path.join(DATA_DIR, \"aclImdb_v1.tar.gz\")\n",
    "\n",
    "if not os.path.exists(source):\n",
    "    !wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P $DATA_DIR\n",
    "\n",
    "if not os.path.exists(\"/tmp/aclImdb\"):\n",
    "    !tar -xf $source -C /tmp\n",
    "\n",
    "train_dir = \"/tmp/aclImdb/train\"\n",
    "test_dir = \"/tmp/aclImdb/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4eb83b8-454e-473d-9f8e-0fdefb113ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(path):\n",
    "    review = []\n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        files = os.listdir(os.path.join(path, label))\n",
    "        for file in files:\n",
    "            with open(os.path.join(path, label, file)) as f:\n",
    "                text = f.readline()\n",
    "                rating = file.split(\".\")[0].split(\"_\")[1]\n",
    "                review.append([text, label, int(rating)])\n",
    "    df = pd.DataFrame(review, columns=[\"text\", \"label\", \"rating\"])\n",
    "    return df\n",
    "\n",
    "train_df = extract_data(train_dir)\n",
    "test_df = extract_data(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b00764b6-3929-4c34-943c-56cb9b40aca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20000, 3), (5000, 3), (25000, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ImdbDataset():\n",
    "    def __init__(self, vocab=None):\n",
    "        super().__init__()\n",
    "        self.tokenizer = Tokenizer(lowercasing=True, separator_type='BySense', token_types=['Word', 'Number'])\n",
    "        nltk.download([\"stopwords\", \"wordnet\"], quiet=True)\n",
    "        self.stop_words = stopwords.words(\"english\")\n",
    "        self.lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "    def _stop_words_filter(self, tokens):\n",
    "        return list(filter(lambda x: x not in self.stop_words, tokens))\n",
    "\n",
    "    def _lemmatize(self, tokens):\n",
    "        return list(map(lambda t: self.lemmatizer.lemmatize(t), tokens))\n",
    "\n",
    "    def preprocess_text(self, texts):\n",
    "        tokenized_text = [self.tokenizer.tokenize(text) for text in texts]\n",
    "        tokenized_no_stop = [self._stop_words_filter(tokens) for tokens in tokenized_text]\n",
    "        lemmatized_text = [\" \".join(self._lemmatize(tokens)) for tokens in tokenized_no_stop]\n",
    "        return lemmatized_text\n",
    "\n",
    "    def from_df(self, data):\n",
    "        return pd.DataFrame({\n",
    "            \"text\": self.preprocess_text(data.text),\n",
    "            \"label\": data.label,\n",
    "            \"rating\": data.rating,\n",
    "        })\n",
    "\n",
    "dataset = ImdbDataset()\n",
    "\n",
    "train_df = dataset.from_df(train_df)\n",
    "test_df = dataset.from_df(test_df)\n",
    "\n",
    "train_df, \\\n",
    "val_df = train_test_split(train_df, stratify=train_df.label, test_size=0.2, random_state=SEED)\n",
    "train_df.shape, val_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac647cd5-e8ae-4bdb-8c0b-729ac4127c05",
   "metadata": {},
   "source": [
    "## Classifier\n",
    "### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0ab5adf-79a6-451a-9bc4-cdaf984c85bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.0552\n",
      "0:\tlearn: 0.8524500\ttest: 0.8596000\tbest: 0.8596000 (0)\ttotal: 143ms\tremaining: 2m 22s\n",
      "100:\tlearn: 0.8738000\ttest: 0.8760000\tbest: 0.8760000 (98)\ttotal: 3.55s\tremaining: 31.6s\n",
      "200:\tlearn: 0.8847000\ttest: 0.8826000\tbest: 0.8826000 (200)\ttotal: 7.19s\tremaining: 28.6s\n",
      "300:\tlearn: 0.8935500\ttest: 0.8878000\tbest: 0.8878000 (300)\ttotal: 10.4s\tremaining: 24.1s\n",
      "400:\tlearn: 0.9005000\ttest: 0.8920000\tbest: 0.8922000 (391)\ttotal: 13.5s\tremaining: 20.2s\n",
      "500:\tlearn: 0.9065500\ttest: 0.8942000\tbest: 0.8944000 (483)\ttotal: 16.5s\tremaining: 16.5s\n",
      "600:\tlearn: 0.9119000\ttest: 0.8978000\tbest: 0.8978000 (599)\ttotal: 19.5s\tremaining: 13s\n",
      "700:\tlearn: 0.9153500\ttest: 0.8988000\tbest: 0.8988000 (658)\ttotal: 22.5s\tremaining: 9.61s\n",
      "800:\tlearn: 0.9189500\ttest: 0.9000000\tbest: 0.9000000 (797)\ttotal: 25.5s\tremaining: 6.34s\n",
      "900:\tlearn: 0.9231500\ttest: 0.8994000\tbest: 0.9004000 (860)\ttotal: 28.6s\tremaining: 3.14s\n",
      "999:\tlearn: 0.9265500\ttest: 0.8994000\tbest: 0.9004000 (860)\ttotal: 31.5s\tremaining: 0us\n",
      "bestTest = 0.9004\n",
      "bestIteration = 860\n",
      "Shrink model to first 861 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.87556"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pool = Pool(\n",
    "    train_df[[\"text\"]], \n",
    "    train_df.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "val_pool = Pool(\n",
    "    val_df[[\"text\"]],\n",
    "    val_df.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "test_pool = Pool(\n",
    "    test_df[[\"text\"]],\n",
    "    test_df.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "clf = CatBoostClassifier(\n",
    "    eval_metric=\"Accuracy\", \n",
    "    task_type=\"GPU\",\n",
    "    random_seed=SEED, \n",
    ")\n",
    "clf.fit(train_pool, eval_set=val_pool, verbose=100)    \n",
    "clf.score(test_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b32b24-3636-4caa-b743-d59b400cfca0",
   "metadata": {},
   "source": [
    "### Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e459efdc-f725-4019-86eb-1b371cfcf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "import torch\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72695073-ceee-4af9-b52f-25237274495c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c523399dd0814a2dafcf5382baf15917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c71270ce72744cf59d184f859affdf4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bba1799be5406aa95654aa663485b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6f9834ad6348b992723e7d652d5965",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"lvwerra/distilbert-imdb\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, clean_up_tokenization_spaces=True)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    label_mapping = {\"neg\": 0, \"pos\": 1}\n",
    "    inputs = tokenizer(examples[\"text\"], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    label = [label_mapping[label] for label in examples[\"label\"]]\n",
    "    return Dataset.from_dict({\n",
    "        \"text\": examples[\"text\"],\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"label\": label,\n",
    "        \"rating\": examples[\"rating\"],\n",
    "    })\n",
    "train_ds = preprocess_function(Dataset.from_dict(train_df))\n",
    "val_ds = preprocess_function(Dataset.from_dict(val_df))\n",
    "test_ds = preprocess_function(Dataset.from_dict(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffab8fe6-0873-428b-91bf-43a759957b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data, bs=4, device=DEVICE):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor(data[\"input_ids\"]).to(device)\n",
    "    attention_mask = torch.tensor(data[\"attention_mask\"]).to(device)\n",
    "    logits = []\n",
    "    for i in tqdm(range(0, len(data), bs)):\n",
    "        batch = range(i, i+bs)\n",
    "        out = model(input_ids[batch], attention_mask=attention_mask[batch])\n",
    "        logits.append(out.logits.data.cpu().detach().numpy())\n",
    "    return np.vstack(logits), data[\"label\"]\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    accuracy = evaluate.load(\"accuracy\")\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb81c64-ee0d-4d4d-a595-2f7ba9f382a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c653a54b7b84cc6832ac9f493e3feaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1233f63264047a9b19c215dc2162846",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40f10560dad54e2a87851ce63b0ba204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02190f8b64e24e2487fd1b0af9e4e046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.84876}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "compute_metrics(predict(model, test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bbcd787-94cb-42aa-a6a0-a7866f1d7818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10000' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10000/10000 24:08, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.313200</td>\n",
       "      <td>0.301227</td>\n",
       "      <td>0.895400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.281400</td>\n",
       "      <td>0.299281</td>\n",
       "      <td>0.908400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.34711530804634094,\n",
       " 'eval_accuracy': 0.89792,\n",
       " 'eval_runtime': 571.2856,\n",
       " 'eval_samples_per_second': 43.761,\n",
       " 'eval_steps_per_second': 10.94,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "modules = [\n",
    "    model.classifier,\n",
    "    model.pre_classifier,\n",
    "    model.distilbert.transformer.layer[-1:],\n",
    "]\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for module in modules:\n",
    "    for param in module.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=\"bert_results\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        num_train_epochs=2,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "08f9688a-a491-4101-b699-9a5230caafaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(os.path.join(MODEL_DIR, \"hf_clf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b164b-edf4-4ebc-afe0-5fb01740bbaf",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23db74ed-f557-42d8-af33-9103cbdceb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.0552\n",
      "0:\tlearn: 0.9197000\ttest: 0.9072000\tbest: 0.9072000 (0)\ttotal: 35.6ms\tremaining: 35.5s\n",
      "100:\tlearn: 0.9280500\ttest: 0.9154000\tbest: 0.9158000 (78)\ttotal: 3.08s\tremaining: 27.5s\n",
      "200:\tlearn: 0.9317000\ttest: 0.9150000\tbest: 0.9162000 (106)\ttotal: 6.23s\tremaining: 24.8s\n",
      "300:\tlearn: 0.9351000\ttest: 0.9158000\tbest: 0.9162000 (106)\ttotal: 9.24s\tremaining: 21.5s\n",
      "400:\tlearn: 0.9374000\ttest: 0.9160000\tbest: 0.9166000 (380)\ttotal: 12.3s\tremaining: 18.3s\n",
      "500:\tlearn: 0.9406000\ttest: 0.9168000\tbest: 0.9170000 (490)\ttotal: 15.2s\tremaining: 15.2s\n",
      "600:\tlearn: 0.9431500\ttest: 0.9172000\tbest: 0.9174000 (589)\ttotal: 18.2s\tremaining: 12.1s\n",
      "700:\tlearn: 0.9454000\ttest: 0.9170000\tbest: 0.9178000 (632)\ttotal: 21.2s\tremaining: 9.05s\n",
      "800:\tlearn: 0.9470500\ttest: 0.9170000\tbest: 0.9178000 (632)\ttotal: 24.2s\tremaining: 6.02s\n",
      "900:\tlearn: 0.9492500\ttest: 0.9176000\tbest: 0.9178000 (632)\ttotal: 27.2s\tremaining: 2.99s\n",
      "999:\tlearn: 0.9509000\ttest: 0.9172000\tbest: 0.9178000 (632)\ttotal: 30.2s\tremaining: 0us\n",
      "bestTest = 0.9178\n",
      "bestIteration = 632\n",
      "Shrink model to first 633 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.90568"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_df.assign(logits=trainer.predict(train_ds).predictions[:,0])\n",
    "val = val_df.assign(logits=trainer.predict(val_ds).predictions[:,0])\n",
    "test = test_df.assign(logits=trainer.predict(test_ds).predictions[:,0])\n",
    "\n",
    "train_pool = Pool(\n",
    "    train[[\"text\", \"logits\"]], \n",
    "    train.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "val_pool = Pool(\n",
    "    val[[\"text\", \"logits\"]],\n",
    "    val.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "test_pool = Pool(\n",
    "    test[[\"text\", \"logits\"]],\n",
    "    test.label, \n",
    "    text_features=[\"text\"],\n",
    ")\n",
    "clf = CatBoostClassifier(\n",
    "    eval_metric=\"Accuracy\", \n",
    "    task_type=\"GPU\",\n",
    "    random_seed=SEED, \n",
    ")\n",
    "clf.fit(train_pool, eval_set=val_pool, verbose=100)    \n",
    "clf.score(test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60b633f6-4fbc-4972-9d65-e4c2101151ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_DIR, \"cb_clf.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(clf, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f34cb-0050-4d2b-a92b-004fd30c95b4",
   "metadata": {},
   "source": [
    "## Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e8bf0ea-f09c-49f7-b4c0-fe46011bf628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65d4c4d367bf47d0a249c027b6819c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755df154d0bb4e338420bcd2b1e1e990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8ee09f378f4e798871ff3b08eda912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% GPU memory available for training. Free: 2671.4375 Total: 5933.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.085827\n",
      "0:\tlearn: 3.2400829\ttest: 3.2300987\tbest: 3.2300987 (0)\ttotal: 20.6ms\tremaining: 20.6s\n",
      "100:\tlearn: 1.3818951\ttest: 1.6833442\tbest: 1.6833442 (100)\ttotal: 1.69s\tremaining: 15s\n",
      "200:\tlearn: 1.3375295\ttest: 1.6800193\tbest: 1.6798837 (197)\ttotal: 3.3s\tremaining: 13.1s\n",
      "300:\tlearn: 1.3110882\ttest: 1.6783720\tbest: 1.6780603 (275)\ttotal: 4.85s\tremaining: 11.3s\n",
      "400:\tlearn: 1.2901631\ttest: 1.6789310\tbest: 1.6779108 (313)\ttotal: 6.4s\tremaining: 9.56s\n",
      "500:\tlearn: 1.2687693\ttest: 1.6798937\tbest: 1.6779108 (313)\ttotal: 7.98s\tremaining: 7.94s\n",
      "600:\tlearn: 1.2492149\ttest: 1.6804701\tbest: 1.6779108 (313)\ttotal: 9.52s\tremaining: 6.32s\n",
      "700:\tlearn: 1.2317313\ttest: 1.6816909\tbest: 1.6779108 (313)\ttotal: 11.1s\tremaining: 4.74s\n",
      "800:\tlearn: 1.2173295\ttest: 1.6808316\tbest: 1.6779108 (313)\ttotal: 12.7s\tremaining: 3.14s\n",
      "900:\tlearn: 1.2034216\ttest: 1.6829679\tbest: 1.6779108 (313)\ttotal: 14.2s\tremaining: 1.56s\n",
      "999:\tlearn: 1.1914413\ttest: 1.6824874\tbest: 1.6779108 (313)\ttotal: 15.7s\tremaining: 0us\n",
      "bestTest = 1.677910834\n",
      "bestIteration = 313\n",
      "Shrink model to first 314 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7144155279856985"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = os.path.join(MODEL_DIR, \"hf_clf\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "with open(os.path.join(MODEL_DIR, \"cb_clf.pkl\"), \"rb\") as f:\n",
    "    clf = pickle.load(f)\n",
    "\n",
    "def predict_proba(ds):\n",
    "    return ds.to_pandas().drop([\"input_ids\", \"attention_mask\"], axis=1) \\\n",
    "    .assign(logits=predict(model, ds)[0][:,0]) \\\n",
    "    .assign(proba=lambda x: clf.predict_proba(x)[:,0])\n",
    "\n",
    "train = predict_proba(train_ds)\n",
    "val = predict_proba(val_ds)\n",
    "test = predict_proba(test_ds)\n",
    "\n",
    "train_pool = Pool(\n",
    "    train.drop([\"label\", \"rating\"], axis=1), \n",
    "    train.rating, \n",
    "    text_features=[\"text\"]\n",
    ")\n",
    "val_pool = Pool(\n",
    "    val.drop([\"label\", \"rating\"], axis=1), \n",
    "    val.rating, \n",
    "    text_features=[\"text\"]\n",
    ")\n",
    "test_pool = Pool(\n",
    "    test.drop([\"label\", \"rating\"], axis=1), \n",
    "    test.rating, \n",
    "    text_features=[\"text\"]\n",
    ")\n",
    "reg = CatBoostRegressor(\n",
    "    objective='RMSE', \n",
    "    task_type=\"GPU\",\n",
    "    random_seed=SEED, \n",
    ")\n",
    "reg.fit(train_pool, eval_set=val_pool, verbose=100)    \n",
    "reg.score(test_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eae5bf35-1462-4745-894e-9d4cd2e48c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_DIR, \"cb_reg.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(reg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b64bd5-6bd5-4e9f-8e52-b8e6327b1bf6",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c69516f8-d19d-45db-ade2-4a99e183f621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    import os\n",
    "    from catboost.text_processing import Tokenizer\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords, wordnet\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "\n",
    "    MODEL_DIR = \"../models\"\n",
    "    \n",
    "    cb_tokenizer = Tokenizer(lowercasing=True, separator_type='BySense', token_types=['Word', 'Number'])\n",
    "    stop_words = stopwords.words(\"english\")\n",
    "    lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "    text = \" \".join([lemmatizer.lemmatize(token) for token in cb_tokenizer.tokenize(text) if token not in stop_words])\n",
    "    \n",
    "    hf_tokenizer = AutoTokenizer.from_pretrained(\"lvwerra/distilbert-imdb\", clean_up_tokenization_spaces=True)\n",
    "    inputs = hf_tokenizer(text, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    \n",
    "    checkpoint = os.path.join(MODEL_DIR, \"hf_clf\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "    model.eval()\n",
    "    logits = model(inputs[\"input_ids\"], inputs[\"attention_mask\"]).logits[:,0].detach().numpy()\n",
    "    \n",
    "    with open(os.path.join(MODEL_DIR, \"cb_clf.pkl\"), \"rb\") as f:\n",
    "        clf = pickle.load(f)\n",
    "    with open(os.path.join(MODEL_DIR, \"cb_reg.pkl\"), \"rb\") as f:\n",
    "        reg = pickle.load(f)\n",
    "\n",
    "    df = pd.DataFrame({\"text\": text, \"logits\": logits}).assign(proba=lambda x: clf.predict_proba(x)[:,0])\n",
    "    \n",
    "    return clf.predict(df)[0], round(reg.predict(df)[0], 0).astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92e0d75b-5b04-47a6-b909-ab8c58af9da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('pos', 8)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"\"\"\n",
    "I went and saw this movie last night after being coaxed to by a few friends of mine. \n",
    "I'll admit that I was reluctant to see it because from what I knew of Ashton Kutcher he was only able to do comedy. I was wrong. \n",
    "Kutcher played the character of Jake Fischer very well, and Kevin Costner played Ben Randall with such professionalism. \n",
    "The sign of a good movie is that it can toy with our emotions. This one did exactly that. \n",
    "The entire theater (which was sold out) was overcome by laughter during the first half of the movie, and were moved to tears during the second half. \n",
    "While exiting the theater I not only saw many women in tears, but many full grown men as well, trying desperately not to let anyone see them crying. \n",
    "This movie was great, and I suggest that you go see it before you judge.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cac4d8d7-27a0-4f8a-99ab-b224ed2c9f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ktc/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "import transformers\n",
    "from transformers import BertTokenizer, AutoConfig, AutoModel, modeling_outputs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "\n",
    "MODEL_DIR = \"../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec8fbdb7-dd14-4e76-a6ad-8858ecfd69d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_DIR, \"vectorizer.pkl\"), \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "with open(os.path.join(MODEL_DIR, \"label_encoder.pkl\"), \"rb\") as f:\n",
    "    le = pickle.load(f)\n",
    "with open(os.path.join(MODEL_DIR, \"clf.pkl\"), \"rb\") as f:\n",
    "    clf = pickle.load(f)\n",
    "with open(os.path.join(MODEL_DIR, \"reg.pkl\"), \"rb\") as f:\n",
    "    reg = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e876841e-e298-4369-a48c-d0f2c77906a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_prediction(text):\n",
    "    X = vectorizer.transform([text])\n",
    "    label = le.inverse_transform(clf.predict(X))[0]\n",
    "    rating = np.around(reg.predict(X)).astype(int)[0]\n",
    "    return label, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3ac471c-234c-4d88-816a-fc46833fdca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('pos', 7), ('neg', 6))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_prediction(\"I would give it a 20 if I could!\"), \\\n",
    "baseline_prediction(\"Just boring...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8d71451-af4b-4adc-ba10-cbf1c395b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImdbDataset():\n",
    "    def __init__(self, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def _lemmatizer(self, doc):\n",
    "        doc = \" \".join(str(doc).split())\n",
    "        doc = [token.lemma_ for token in nlp(doc) if token.lemma_ not in stop_words]\n",
    "        return \" \".join(doc)\n",
    "\n",
    "    def _lemmatize(self, text):\n",
    "        text = text.apply(self._lemmatizer)\n",
    "        return text\n",
    "\n",
    "    def _label_encoder(self, labels):\n",
    "        return [1 if label == \"pos\" else 0 for label in labels]\n",
    "\n",
    "    def _preprocess_function(self, batch, tokenizer, max_length):\n",
    "        return tokenizer(batch['text'], truncation=True, max_length=max_length)\n",
    "\n",
    "    def from_df(self, data):\n",
    "        data = Dataset.from_dict({\n",
    "            \"text\": self._lemmatize(data[\"text\"]), \n",
    "            \"labels\": self._label_encoder(data[\"label\"]),\n",
    "            \"rating\": data[\"rating\"]}\n",
    "        )\n",
    "        tokenized_data = data.map(\n",
    "            self._preprocess_function, \n",
    "            batched=True, \n",
    "            fn_kwargs={\"tokenizer\": self.tokenizer, \"max_length\": 512}\n",
    "        )\n",
    "        tokenized_data = tokenized_data.class_encode_column(\"labels\")\n",
    "        return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0560e76-d5c3-46db-8bd0-212ea526e650",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, hid_dim=1024):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_channels, hid_dim),\n",
    "            nn.LayerNorm(hid_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hid_dim, hid_dim),\n",
    "            nn.LayerNorm(hid_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hid_dim, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class ImdbBertClassifier(nn.Module):\n",
    "    def __init__(self, out_features):\n",
    "        super(ImdbBertClassifier, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.config = AutoConfig.from_pretrained(checkpoint, output_attentions=True, attn_implementation=\"eager\")\n",
    "        self.backbone = AutoModel.from_pretrained(checkpoint, config=self.config)\n",
    "        in_features = self.backbone.pooler.dense.out_features\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = Classifier(in_features, out_features)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out = self.dropout(out.pooler_output)\n",
    "        logits = self.classifier(out)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "        out = modeling_outputs.TokenClassifierOutput({\"loss\": loss, \"logits\": logits})\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ab1b743-4c61-432f-9ed4-83c11f63329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_label(path, input_ids, attention_mask):\n",
    "    model = torch.load(path, weights_only=False)\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    preds = model(input_ids, attention_mask)[0].detach().numpy()\n",
    "    label = \"pos\" if np.argmax(preds, axis=1) else \"neg\"\n",
    "    return label\n",
    "\n",
    "\n",
    "def predict_rating(path, input_ids, attention_mask, rating_ids=[1, 10, 2, 3, 4, 7, 8, 9]):\n",
    "    model = torch.load(path, weights_only=False)\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    preds = model(input_ids, attention_mask)[0].detach().numpy()\n",
    "    rating = rating_ids[np.argmax(preds, axis=1)[0]]\n",
    "    return rating\n",
    "\n",
    "def predict(text):\n",
    "    checkpoint = \"bert-base-uncased\"\n",
    "    tokenizer = BertTokenizer.from_pretrained(checkpoint, clean_up_tokenization_spaces=True)\n",
    "    data_collator = transformers.DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    dataset = ImdbDataset(tokenizer)\n",
    "    ds = dataset.from_df(pd.DataFrame({\"text\": [text], \"label\": [None], \"rating\": [None]}))\n",
    "    batch = next(iter(ds))\n",
    "    input_ids = torch.tensor(batch[\"input_ids\"]).unsqueeze(0)\n",
    "    attention_mask = torch.tensor(batch[\"attention_mask\"]).unsqueeze(0)\n",
    "    label = predict_label(\n",
    "        os.path.join(MODEL_DIR, \"bert_clf2.pt\"), input_ids, attention_mask\n",
    "    )\n",
    "    rating = predict_rating(\n",
    "        os.path.join(MODEL_DIR, \"bert_clf8.pt\"), input_ids, attention_mask\n",
    "    )\n",
    "    return label, rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "684ce445-abc2-4ce9-93bc-a64f1dc5e838",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c20019112c14ff5a63aa442a7d7334c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a662e43643954ecebae2d1fe751d6de9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfcd01ad742544ab906eccc09e2e6ba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('pos', 10)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f140b748ed69462a9a07787083c523c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d73d7f28528d4c9c8d20003b514e86fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Stringifying the column:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272d141d833f473293c293e3f7b2d600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting to class labels:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('neg', 4)\n"
     ]
    }
   ],
   "source": [
    "review_pos = \"\"\"\n",
    "My boyfriend and I went to watch The Guardian.At first I didn't want to watch it, but I loved the movie- It was definitely \n",
    "the best movie I have seen in sometime.They portrayed the USCG very well, it really showed me what they do \n",
    "and I think they should really be appreciated more.Not only did it teach but it was a really good movie. The movie shows \n",
    "what the really do and how hard the job is.I think being a USCG would be challenging and very scary. It was a great movie all around. \n",
    "I would suggest this movie for anyone to see.The ending broke my heart but I know why he did it. The storyline was great \n",
    "I give it 2 thumbs up. I cried it was very emotional, I would give it a 20 if I could!\n",
    "\"\"\"\n",
    "review_neg = \"\"\"\n",
    "This is a pale imitation of 'Officer and a Gentleman.' There is NO chemistry between Kutcher and the unknown woman \n",
    "who plays his love interest. The dialog is wooden, the situations hackneyed. It's too long and the climax is anti-climactic(!). \n",
    "I love the USCG, its men and women are fearless and tough. The action scenes are awesome, but this movie doesn't do much \n",
    "for recruiting, I fear. The script is formulaic, but confusing. Kutcher's character is trying to redeem himself for an accident \n",
    "that wasn't his fault? Costner's is raging against the dying of the light, but why? His 'conflict' with his wife is about as deep \n",
    "as a mud puddle. I saw this sneak preview for free and certainly felt I got my money's worth.\n",
    "\"\"\"\n",
    "print(predict(review_pos))\n",
    "print(predict(review_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ec170-3167-4002-8665-3fabd9c2750a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
